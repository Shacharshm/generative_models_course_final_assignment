# LLMs Safeguard: Inference Guidance Away From Harmful Content
Shachar Shmueli  
Ori Kenig  
Evyatar Ozeri  
Semion Verzhik  

## Abstract
Large Language Models (LLMs) are powerful tools for generating new data by analyzing vast amounts of text and identifying patterns. A key strength is their ability to be "creative," enabling them to generate almost anything within the output space from human-like dialogue and stories to technical documents and synthetic datasets. This creative capability fuels innovation in areas like customer support, content generation, and AI training. However, this versatility also introduces risks. Adversarial actors can exploit LLMs to generate harmful content, including disinformation, hate speech, or phishing scams, and may attempt to "jailbreak" models, bypassing built-in safety measures to elicit inappropriate or dangerous outputs. Balancing creativity with robust safeguards is critical to responsibly harness their potential while mitigating these threats.

In this project, we propose an enhanced inference mechanism for LLMs aimed at mitigating the generation of harmful content. Our approach leverages the knowledge already embedded within the model, eliminating the need for retraining. This ensures seamless integration with existing LLMs, offering a practical and efficient solution for reducing adversarial misuse without compromising performance or requiring significant modifications to current deployments.

We anticipate that integrating our method into LLMs will enhance their ability to resist harmful user requests, making them more secure and safer for deployment in various applications.


